{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIBXviuDMSFxt4SKriYgEk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nsambel1980/causal_discovery/blob/main/Causal_discovery_signature_public.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "L5pHkBq-Q7OH",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Causal discovery based on Taken's embedding and signature proximity.\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "from itertools import combinations\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "def normalize_timeseries(timeseries):\n",
        "    \"\"\"\n",
        "    Normalize time series to prevent numerical issues.\n",
        "    \"\"\"\n",
        "    # Remove any infinite values\n",
        "    timeseries = np.nan_to_num(timeseries, nan=0.0, posinf=None, neginf=None)\n",
        "\n",
        "    # Standardize to zero mean and unit variance\n",
        "    scaler = StandardScaler()\n",
        "    normalized = scaler.fit_transform(timeseries.reshape(-1, 1)).ravel()\n",
        "\n",
        "    return normalized\n",
        "\n",
        "def create_embedding(timeseries, embedding_dimension, tau=1):\n",
        "    \"\"\"\n",
        "    Create time-delay embedding of a time series using Takens' theorem.\n",
        "    \"\"\"\n",
        "    # Normalize input\n",
        "    timeseries = normalize_timeseries(timeseries)\n",
        "\n",
        "    N = len(timeseries)\n",
        "    if embedding_dimension * tau > N:\n",
        "        raise ValueError(\"Embedding dimension too large for time series length\")\n",
        "\n",
        "    embedded = np.zeros((N - (embedding_dimension-1)*tau, embedding_dimension))\n",
        "\n",
        "    for i in range(embedding_dimension):\n",
        "        embedded[:, i] = timeseries[i*tau:N-(embedding_dimension-1-i)*tau]\n",
        "\n",
        "    return embedded\n",
        "\n",
        "def compute_signature_features(embedded_ts, order=2):\n",
        "    \"\"\"\n",
        "    Compute signature features from embedded time series with numerical stability.\n",
        "    \"\"\"\n",
        "    n_samples, dim = embedded_ts.shape\n",
        "    features = []\n",
        "\n",
        "    # Add raw coordinates\n",
        "    features.append(embedded_ts)\n",
        "\n",
        "    try:\n",
        "        # Add higher order terms with numerical stability\n",
        "        for k in range(2, order + 1):\n",
        "            for indices in combinations(range(dim), k):\n",
        "                prod = np.ones(n_samples)\n",
        "                for idx in indices:\n",
        "                    # Use log-sum-exp trick for numerical stability\n",
        "                    prod = np.exp(np.log(np.abs(prod)) + np.log(np.abs(embedded_ts[:, idx])))\n",
        "                    prod = np.sign(prod) * np.sign(embedded_ts[:, idx])\n",
        "                features.append(prod.reshape(-1, 1))\n",
        "    except (RuntimeWarning, RuntimeError) as e:\n",
        "        warnings.warn(f\"Numerical stability issue in signature computation: {str(e)}\")\n",
        "        # Fall back to just using the raw embeddings\n",
        "        return embedded_ts\n",
        "\n",
        "    result = np.hstack(features)\n",
        "    # Final normalization\n",
        "    result = normalize_timeseries(result.ravel()).reshape(result.shape)\n",
        "    return result\n",
        "\n",
        "def ccm_signature_causality(X, Y, embedding_dimension=3, tau=1, n_neighbors=5,\n",
        "                          signature_order=2, prediction_horizon=1):\n",
        "    \"\"\"\n",
        "    Assess causality between time series using CCM with embeddings and signatures.\n",
        "    \"\"\"\n",
        "    # Input validation and normalization\n",
        "    X = normalize_timeseries(X)\n",
        "    Y = normalize_timeseries(Y)\n",
        "\n",
        "    try:\n",
        "        # Create embeddings\n",
        "        X_embedded = create_embedding(X, embedding_dimension, tau)\n",
        "        Y_embedded = create_embedding(Y, embedding_dimension, tau)\n",
        "\n",
        "        # Compute signature features with error handling\n",
        "        try:\n",
        "            X_sig = compute_signature_features(X_embedded, signature_order)\n",
        "            Y_sig = compute_signature_features(Y_embedded, signature_order)\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"Falling back to raw embeddings due to: {str(e)}\")\n",
        "            X_sig = X_embedded\n",
        "            Y_sig = Y_embedded\n",
        "\n",
        "        # Initialize nearest neighbors models\n",
        "        X_nn = NearestNeighbors(n_neighbors=min(n_neighbors, len(X_sig)-1))\n",
        "        Y_nn = NearestNeighbors(n_neighbors=min(n_neighbors, len(Y_sig)-1))\n",
        "\n",
        "        X_nn.fit(X_sig)\n",
        "        Y_nn.fit(Y_sig)\n",
        "\n",
        "        # Predict Y using X's manifold\n",
        "        X_to_Y_pred = np.zeros(len(Y) - prediction_horizon - (embedding_dimension-1)*tau)\n",
        "        Y_to_X_pred = np.zeros(len(X) - prediction_horizon - (embedding_dimension-1)*tau)\n",
        "\n",
        "        # Cross mapping with error handling\n",
        "        for i in range(len(X_to_Y_pred)):\n",
        "            try:\n",
        "                distances, indices = X_nn.kneighbors([X_sig[i]], n_neighbors)\n",
        "                weights = np.exp(-distances[0])\n",
        "                weights /= np.sum(weights) + 1e-10  # Avoid division by zero\n",
        "                future_idx = i + prediction_horizon\n",
        "                if future_idx < len(Y):\n",
        "                    X_to_Y_pred[i] = np.sum(weights * Y[indices[0] + prediction_horizon])\n",
        "            except Exception as e:\n",
        "                warnings.warn(f\"Error in X->Y prediction at index {i}: {str(e)}\")\n",
        "                X_to_Y_pred[i] = np.nan\n",
        "\n",
        "        for i in range(len(Y_to_X_pred)):\n",
        "            try:\n",
        "                distances, indices = Y_nn.kneighbors([Y_sig[i]], n_neighbors)\n",
        "                weights = np.exp(-distances[0])\n",
        "                weights /= np.sum(weights) + 1e-10\n",
        "                future_idx = i + prediction_horizon\n",
        "                if future_idx < len(X):\n",
        "                    Y_to_X_pred[i] = np.sum(weights * X[indices[0] + prediction_horizon])\n",
        "            except Exception as e:\n",
        "                warnings.warn(f\"Error in Y->X prediction at index {i}: {str(e)}\")\n",
        "                Y_to_X_pred[i] = np.nan\n",
        "\n",
        "        # Remove NaN values\n",
        "        mask = ~np.isnan(X_to_Y_pred) & ~np.isnan(Y_to_X_pred)\n",
        "        X_to_Y_pred = X_to_Y_pred[mask]\n",
        "        Y_to_X_pred = Y_to_X_pred[mask]\n",
        "\n",
        "        # Compute correlations with error handling\n",
        "        try:\n",
        "            X_to_Y_corr = np.corrcoef(X_to_Y_pred,\n",
        "                                     Y[prediction_horizon + (embedding_dimension-1)*tau:][:len(X_to_Y_pred)])[0,1]\n",
        "            Y_to_X_corr = np.corrcoef(Y_to_X_pred,\n",
        "                                     X[prediction_horizon + (embedding_dimension-1)*tau:][:len(Y_to_X_pred)])[0,1]\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"Error computing correlations: {str(e)}\")\n",
        "            X_to_Y_corr = np.nan\n",
        "            Y_to_X_corr = np.nan\n",
        "\n",
        "        # Compute RMSE\n",
        "        X_to_Y_rmse = np.sqrt(np.mean((X_to_Y_pred -\n",
        "                                      Y[prediction_horizon + (embedding_dimension-1)*tau:][:len(X_to_Y_pred)])**2))\n",
        "        Y_to_X_rmse = np.sqrt(np.mean((Y_to_X_pred -\n",
        "                                      X[prediction_horizon + (embedding_dimension-1)*tau:][:len(Y_to_X_pred)])**2))\n",
        "\n",
        "        results = {\n",
        "            'X_causes_Y': X_to_Y_corr,\n",
        "            'Y_causes_X': Y_to_X_corr,\n",
        "            'X_to_Y_rmse': X_to_Y_rmse,\n",
        "            'Y_to_X_rmse': Y_to_X_rmse,\n",
        "            'predictions': {\n",
        "                'X_to_Y': X_to_Y_pred,\n",
        "                'Y_to_X': Y_to_X_pred\n",
        "            },\n",
        "            'embedding_info': {\n",
        "                'dimension': embedding_dimension,\n",
        "                'tau': tau,\n",
        "                'signature_order': signature_order\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error in causality analysis: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6TPATXS3Wa6q",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Computes statistical singificance of the results by permutation\n",
        "def compute_statistical_significance(X, Y, results, n_permutations=100, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Compute statistical significance of causal relationships using permutation tests.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X, Y : array-like\n",
        "        Original time series\n",
        "    results : dict\n",
        "        Results from ccm_signature_causality\n",
        "    n_permutations : int\n",
        "        Number of permutations for null distribution\n",
        "    alpha : float\n",
        "        Significance level\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Dictionary containing significance test results\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    from scipy import stats\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    # Store original correlation values\n",
        "    orig_xy_corr = results['X_causes_Y']\n",
        "    orig_yx_corr = results['Y_causes_X']\n",
        "\n",
        "    # Initialize arrays for null distributions\n",
        "    xy_null_dist = np.zeros(n_permutations)\n",
        "    yx_null_dist = np.zeros(n_permutations)\n",
        "\n",
        "    # Generate null distributions through permutation\n",
        "    print(\"Generating null distributions...\")\n",
        "    for i in tqdm(range(n_permutations)):\n",
        "        # Randomly permute Y while keeping X fixed\n",
        "        Y_perm = np.random.permutation(Y)\n",
        "\n",
        "        # Compute causality on permuted data\n",
        "        perm_results = ccm_signature_causality(\n",
        "            X=X,\n",
        "            Y=Y_perm,\n",
        "            embedding_dimension=results['embedding_info']['dimension'],\n",
        "            tau=results['embedding_info']['tau'],\n",
        "            signature_order=results['embedding_info']['signature_order']\n",
        "        )\n",
        "\n",
        "        # Store results\n",
        "        xy_null_dist[i] = perm_results['X_causes_Y']\n",
        "        yx_null_dist[i] = perm_results['Y_causes_X']\n",
        "\n",
        "    # Compute p-values\n",
        "    xy_p_value = np.mean(xy_null_dist >= abs(orig_xy_corr))\n",
        "    yx_p_value = np.mean(yx_null_dist >= abs(orig_yx_corr))\n",
        "\n",
        "    # Compute confidence intervals\n",
        "    xy_ci = np.percentile(xy_null_dist, [2.5, 97.5])\n",
        "    yx_ci = np.percentile(yx_null_dist, [2.5, 97.5])\n",
        "\n",
        "    # Compute effect sizes (Cohen's d)\n",
        "    xy_effect_size = (orig_xy_corr - np.mean(xy_null_dist)) / np.std(xy_null_dist)\n",
        "    yx_effect_size = (orig_yx_corr - np.mean(yx_null_dist)) / np.std(yx_null_dist)\n",
        "\n",
        "    # Compute Granger-style F-test\n",
        "    def compute_f_test(actual_corr, null_dist):\n",
        "        variance_ratio = np.var(null_dist) / (np.var([actual_corr]) + 1e-10)  # Add a small constant\n",
        "        f_stat = variance_ratio\n",
        "        f_p_value = 1 - stats.f.cdf(f_stat, len(null_dist)-1, 1)\n",
        "        return f_stat, f_p_value\n",
        "\n",
        "    xy_f_stat, xy_f_p_value = compute_f_test(orig_xy_corr, xy_null_dist)\n",
        "    yx_f_stat, yx_f_p_value = compute_f_test(orig_yx_corr, yx_null_dist)\n",
        "\n",
        "    significance_results = {\n",
        "        'X_causes_Y': {\n",
        "            'p_value': xy_p_value,\n",
        "            'confidence_interval': xy_ci,\n",
        "            'effect_size': xy_effect_size,\n",
        "            'f_statistic': xy_f_stat,\n",
        "            'f_p_value': xy_f_p_value,\n",
        "            'significant': xy_p_value < alpha,\n",
        "            'null_distribution': xy_null_dist\n",
        "        },\n",
        "        'Y_causes_X': {\n",
        "            'p_value': yx_p_value,\n",
        "            'confidence_interval': yx_ci,\n",
        "            'effect_size': yx_effect_size,\n",
        "            'f_statistic': yx_f_stat,\n",
        "            'f_p_value': yx_f_p_value,\n",
        "            'significant': yx_p_value < alpha,\n",
        "            'null_distribution': yx_null_dist\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return significance_results\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot ccm results\n",
        "def plot_ccm_results(X, Y, results, title=\"CCM Causality Results\"):\n",
        "    \"\"\"\n",
        "    Plots the original time series and the cross-mapped predictions.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n",
        "\n",
        "    # Plot original time series\n",
        "    axes[0].plot(X, label='X', color='blue')\n",
        "    axes[0].plot(Y, label='Y', color='red')\n",
        "    axes[0].set_ylabel('Original Values')\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Plot cross-mapped predictions\n",
        "    prediction_horizon = results['embedding_info']['dimension'] * results['embedding_info']['tau']\n",
        "    axes[1].plot(Y[prediction_horizon:][:len(results['predictions']['X_to_Y'])], label='Y (Actual)', color='red', linestyle='--')\n",
        "    axes[1].plot(results['predictions']['X_to_Y'], label='Y (Predicted from X)', color='green')\n",
        "\n",
        "    axes[1].plot(X[prediction_horizon:][:len(results['predictions']['Y_to_X'])], label='X (Actual)', color='blue', linestyle='--')\n",
        "    axes[1].plot(results['predictions']['Y_to_X'], label='X (Predicted from Y)', color='orange')\n",
        "\n",
        "    axes[1].set_ylabel('Cross-Mapped Predictions')\n",
        "    axes[1].set_xlabel('Time')\n",
        "    axes[1].legend()\n",
        "\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pQjGfQOsR6Du"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot significance results\n",
        "def plot_significance_results(results, significance_results):\n",
        "    \"\"\"\n",
        "    Plot statistical significance test results.\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Plot null distributions and actual values for X->Y\n",
        "    sns.histplot(significance_results['X_causes_Y']['null_distribution'],\n",
        "                ax=axes[0,0], label='Null Distribution')\n",
        "    axes[0,0].axvline(results['X_causes_Y'], color='r', linestyle='--',\n",
        "                      label='Observed Value')\n",
        "    axes[0,0].set_title(f\"X→Y Causality\\np-value: {significance_results['X_causes_Y']['p_value']:.3f}\")\n",
        "    axes[0,0].legend()\n",
        "\n",
        "    # Plot null distributions and actual values for Y->X\n",
        "    sns.histplot(significance_results['Y_causes_X']['null_distribution'],\n",
        "                ax=axes[0,1], label='Null Distribution')\n",
        "    axes[0,1].axvline(results['Y_causes_X'], color='r', linestyle='--',\n",
        "                      label='Observed Value')\n",
        "    axes[0,1].set_title(f\"Y→X Causality\\np-value: {significance_results['Y_causes_X']['p_value']:.3f}\")\n",
        "    axes[0,1].legend()\n",
        "\n",
        "    # Plot effect sizes\n",
        "    effect_sizes = [significance_results['X_causes_Y']['effect_size'],\n",
        "                   significance_results['Y_causes_X']['effect_size']]\n",
        "    axes[1,0].bar(['X→Y', 'Y→X'], effect_sizes)\n",
        "    axes[1,0].set_title(\"Effect Sizes (Cohen's d)\")\n",
        "\n",
        "    # Plot confidence intervals\n",
        "    axes[1,1].errorbar(['X→Y', 'Y→X'],\n",
        "                      [results['X_causes_Y'], results['Y_causes_X']],\n",
        "                      yerr=[[np.abs(results['X_causes_Y'] - significance_results['X_causes_Y']['confidence_interval'][0]), # Take absolute value\n",
        "                            np.abs(results['Y_causes_X'] - significance_results['Y_causes_X']['confidence_interval'][0])],\n",
        "                           [np.abs(significance_results['X_causes_Y']['confidence_interval'][1] - results['X_causes_Y']), # Take absolute value\n",
        "                            np.abs(significance_results['Y_causes_X']['confidence_interval'][1] - results['Y_causes_X'])]],\n",
        "                      fmt='o')\n",
        "    axes[1,1].set_title(\"95% Confidence Intervals\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dz_BypUoSGEY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Interpret significance results\n",
        "def interpret_significance_results(significance_results, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Provide interpretation of statistical significance results.\n",
        "    \"\"\"\n",
        "    interpretations = []\n",
        "\n",
        "    # Interpret X->Y relationship\n",
        "    xy_sig = significance_results['X_causes_Y']\n",
        "    interpretations.append(\"X → Y Relationship:\")\n",
        "    interpretations.append(f\"- P-value: {xy_sig['p_value']:.3f}\")\n",
        "    interpretations.append(f\"- Effect size (Cohen's d): {xy_sig['effect_size']:.3f}\")\n",
        "    interpretations.append(f\"- 95% CI: [{xy_sig['confidence_interval'][0]:.3f}, {xy_sig['confidence_interval'][1]:.3f}]\")\n",
        "\n",
        "    if xy_sig['significant']:\n",
        "        effect_size = abs(xy_sig['effect_size'])\n",
        "        if effect_size > 0.8:\n",
        "            strength = \"strong\"\n",
        "        elif effect_size > 0.5:\n",
        "            strength = \"moderate\"\n",
        "        else:\n",
        "            strength = \"weak\"\n",
        "        interpretations.append(f\"- Statistically significant {strength} effect\")\n",
        "    else:\n",
        "        interpretations.append(\"- Not statistically significant\")\n",
        "\n",
        "    # Interpret Y->X relationship\n",
        "    yx_sig = significance_results['Y_causes_X']\n",
        "    interpretations.append(\"\\nY → X Relationship:\")\n",
        "    interpretations.append(f\"- P-value: {yx_sig['p_value']:.3f}\")\n",
        "    interpretations.append(f\"- Effect size (Cohen's d): {yx_sig['effect_size']:.3f}\")\n",
        "    interpretations.append(f\"- 95% CI: [{yx_sig['confidence_interval'][0]:.3f}, {yx_sig['confidence_interval'][1]:.3f}]\")\n",
        "\n",
        "    if yx_sig['significant']:\n",
        "        effect_size = abs(yx_sig['effect_size'])\n",
        "        if effect_size > 0.8:\n",
        "            strength = \"strong\"\n",
        "        elif effect_size > 0.5:\n",
        "            strength = \"moderate\"\n",
        "        else:\n",
        "            strength = \"weak\"\n",
        "        interpretations.append(f\"- Statistically significant {strength} effect\")\n",
        "    else:\n",
        "        interpretations.append(\"- Not statistically significant\")\n",
        "\n",
        "    # Compare directions\n",
        "    if xy_sig['significant'] and yx_sig['significant']:\n",
        "        if abs(xy_sig['effect_size']) > abs(yx_sig['effect_size']):\n",
        "            interpretations.append(\"\\nBoth directions are significant, but X→Y is stronger\")\n",
        "        elif abs(xy_sig['effect_size']) < abs(yx_sig['effect_size']):\n",
        "            interpretations.append(\"\\nBoth directions are significant, but Y→X is stronger\")\n",
        "        else:\n",
        "            interpretations.append(\"\\nBoth directions are significant with similar strength\")\n",
        "    elif xy_sig['significant']:\n",
        "        interpretations.append(\"\\nOnly X→Y is significant\")\n",
        "    elif yx_sig['significant']:\n",
        "        interpretations.append(\"\\nOnly Y→X is significant\")\n",
        "    else:\n",
        "        interpretations.append(\"\\nNo significant causal relationships detected\")\n",
        "\n",
        "    return interpretations"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6gimQu7aSNuM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Main playground\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function with statistical significance testing.\n",
        "    \"\"\"\n",
        "    # Generate test data\n",
        "    print(\"Generating test data...\")\n",
        "    n_points = 1000\n",
        "    t = np.linspace(0, 10, n_points)\n",
        "    # sin function with noise\n",
        "    X = np.sin(t) + 0.1 * np.random.randn(n_points)\n",
        "    # Shift X back by 5 and apply sin function on top\n",
        "    Y = np.sin(np.roll(X, 5)) + 0.2 * np.random.randn(n_points)\n",
        "\n",
        "    # Run causality analysis\n",
        "    print(\"\\nRunning causality analysis...\")\n",
        "    results = ccm_signature_causality(X, Y)\n",
        "\n",
        "    # Compute statistical significance\n",
        "    print(\"\\nComputing statistical significance...\")\n",
        "    significance_results = compute_statistical_significance(X, Y, results)\n",
        "\n",
        "    # Print interpretations\n",
        "    print(\"\\nInterpretation of Results:\")\n",
        "    print(\"-\" * 50)\n",
        "    for interp in interpret_significance_results(significance_results):\n",
        "        print(interp)\n",
        "\n",
        "    # Plot results\n",
        "    plot_ccm_results(X, Y, results)\n",
        "    plt.show()\n",
        "    plot_significance_results(results, significance_results)\n",
        "    plt.show()\n",
        "\n",
        "    return results, significance_results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results, significance_results = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmzA_tzvLQaJ",
        "outputId": "a8e8760c-9e05-4700-c73d-87abea889b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating test data...\n",
            "\n",
            "Running causality analysis...\n",
            "\n",
            "Computing statistical significance...\n",
            "Generating null distributions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 66%|██████▌   | 66/100 [01:25<00:44,  1.31s/it]"
          ]
        }
      ]
    }
  ]
}