{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHdE32pUo2QsruViZNrBKG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nsambel1980/causal_discovery/blob/main/Zipf_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zipfian network"
      ],
      "metadata": {
        "id": "bBrO0_IjEA6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ZipfianLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, frequency_groups=None):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        # Create Zipfian connection mask\n",
        "        mask = torch.zeros(out_features, in_features)\n",
        "        for i in range(out_features):\n",
        "            # Number of connections for this neuron follows Zipf law\n",
        "            n_connections = max(1, int(in_features / (i + 1)))\n",
        "            # Randomly select input neurons to connect\n",
        "            connections = torch.randperm(in_features)[:n_connections]\n",
        "            mask[i, connections] = 1\n",
        "\n",
        "        self.register_buffer('mask', mask)\n",
        "\n",
        "        # Initialize weights with scaled variance based on connections\n",
        "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        connection_counts = mask.sum(1)\n",
        "        std = torch.sqrt(2.0 / connection_counts)\n",
        "        self.weight.data.normal_(0, 1)\n",
        "        for i in range(out_features):\n",
        "            self.weight.data[i] *= std[i]\n",
        "\n",
        "        self.bias = nn.Parameter(torch.Tensor(out_features))\n",
        "        self.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.linear(x, self.weight * self.mask, self.bias)\n",
        "\n",
        "class FinancialZipfNet(nn.Module):\n",
        "    def __init__(self, feature_groups, hidden_sizes=[64, 32, 16], dropout_rate=0.1):\n",
        "        \"\"\"\n",
        "        feature_groups: list of tuples (size, frequency)\n",
        "        e.g., [(100, 'high'), (50, 'medium'), (10, 'low')]\n",
        "        dropout_rate: float, dropout probability (default: 0.1 to maintain Zipfian structure)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_size = sum(size for size, _ in feature_groups)\n",
        "        self.feature_groups = feature_groups\n",
        "\n",
        "        # Create frequency-based input embeddings\n",
        "        self.input_layers = nn.ModuleList()\n",
        "        current_pos = 0\n",
        "        for size, freq in feature_groups:\n",
        "            if freq == 'high':\n",
        "                layer_size = size // 2\n",
        "            elif freq == 'medium':\n",
        "                layer_size = size // 3\n",
        "            else:  # low\n",
        "                layer_size = size // 4\n",
        "\n",
        "            layer_block = nn.Sequential(\n",
        "                ZipfianLayer(size, layer_size),\n",
        "                nn.BatchNorm1d(layer_size),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout_rate)\n",
        "            )\n",
        "            self.input_layers.append(layer_block)\n",
        "            current_pos += size\n",
        "\n",
        "        # Create merged processing layers with residual connections\n",
        "        self.merged_layers = nn.ModuleList()\n",
        "        self.residual_projections = nn.ModuleList()\n",
        "        current_size = sum(size // (2 if freq == 'high' else 3 if freq == 'medium' else 4)\n",
        "                          for size, freq in feature_groups)\n",
        "\n",
        "        for hidden_size in hidden_sizes:\n",
        "            # Residual projection if sizes don't match\n",
        "            if current_size != hidden_size:\n",
        "                self.residual_projections.append(\n",
        "                    nn.Sequential(\n",
        "                        nn.Linear(current_size, hidden_size),\n",
        "                        nn.BatchNorm1d(hidden_size)\n",
        "                    )\n",
        "                )\n",
        "            else:\n",
        "                self.residual_projections.append(nn.Identity())\n",
        "\n",
        "            # Main layer block\n",
        "            layer_block = nn.Sequential(\n",
        "                ZipfianLayer(current_size, hidden_size),\n",
        "                nn.BatchNorm1d(hidden_size),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout_rate)\n",
        "            )\n",
        "            self.merged_layers.append(layer_block)\n",
        "            current_size = hidden_size\n",
        "\n",
        "        # Final prediction layer\n",
        "        self.final_layer = nn.Linear(hidden_sizes[-1], 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Split input by frequency groups\n",
        "        current_pos = 0\n",
        "        processed_groups = []\n",
        "\n",
        "        for (size, _), layer in zip(self.feature_groups, self.input_layers):\n",
        "            group_input = x[:, current_pos:current_pos + size]\n",
        "            processed = layer(group_input)\n",
        "            processed_groups.append(processed)\n",
        "            current_pos += size\n",
        "\n",
        "        # Merge processed groups\n",
        "        merged = torch.cat(processed_groups, dim=1)\n",
        "\n",
        "        # Process through merged layers with residual connections\n",
        "        for layer, residual_proj in zip(self.merged_layers, self.residual_projections):\n",
        "            identity = residual_proj(merged)\n",
        "            merged = layer(merged) + identity\n",
        "\n",
        "        # Final prediction\n",
        "        return self.final_layer(merged)\n",
        "\n",
        "# Example usage and validation\n",
        "def create_synthetic_data(n_samples=1000):\n",
        "    \"\"\"Create synthetic financial data for testing\"\"\"\n",
        "    # High frequency features (e.g., daily prices, volumes)\n",
        "    high_freq = np.random.randn(n_samples, 100)\n",
        "    # Medium frequency features (e.g., weekly indicators)\n",
        "    med_freq = np.random.randn(n_samples, 50)\n",
        "    # Low frequency features (e.g., quarterly reports)\n",
        "    low_freq = np.random.randn(n_samples, 10)\n",
        "\n",
        "    # Create target with known relationships\n",
        "    target = (high_freq[:, :10].mean(axis=1) * 0.5 +  # Strong influence from some high-freq features\n",
        "             med_freq[:, :5].mean(axis=1) * 0.3 +    # Medium influence from med-freq\n",
        "             low_freq[:, :2].mean(axis=1) * 0.2 +    # Small influence from low-freq\n",
        "             np.random.randn(n_samples) * 0.1)       # Noise\n",
        "\n",
        "    return np.hstack([high_freq, med_freq, low_freq]), target\n",
        "\n",
        "def train_and_validate():\n",
        "    # Create synthetic data\n",
        "    X, y = create_synthetic_data()\n",
        "    X = torch.FloatTensor(X)\n",
        "    y = torch.FloatTensor(y).reshape(-1, 1)\n",
        "\n",
        "    # Split data\n",
        "    train_size = int(0.8 * len(X))\n",
        "    X_train, X_test = X[:train_size], X[train_size:]\n",
        "    y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "    # Create model\n",
        "    feature_groups = [(100, 'high'), (50, 'medium'), (10, 'low')]\n",
        "    model = FinancialZipfNet(feature_groups)\n",
        "\n",
        "    # Training setup\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "    # Training loop\n",
        "    n_epochs = 100\n",
        "    batch_size = 32\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        for i in range(0, len(X_train), batch_size):\n",
        "            batch_X = X_train[i:i+batch_size]\n",
        "            batch_y = y_train[i:i+batch_size]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(batch_X)\n",
        "            loss = criterion(pred, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                train_pred = model(X_train)\n",
        "                train_loss = criterion(train_pred, y_train)\n",
        "                test_pred = model(X_test)\n",
        "                test_loss = criterion(test_pred, y_test)\n",
        "                current_lr = optimizer.param_groups[0]['lr']\n",
        "                print(f'Epoch {epoch+1}, Train Loss: {train_loss.item():.4f}, Test Loss: {test_loss.item():.4f}, LR: {current_lr:.6f}')\n",
        "\n",
        "    return model, test_loss.item()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, final_test_loss = train_and_validate()\n",
        "    print(f\"Final test loss: {final_test_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAvEpXI7HQfY",
        "outputId": "32c0b70d-b89b-4344-bd64-10feb5b03209"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Train Loss: 0.0442, Test Loss: 0.0725, LR: 0.001000\n",
            "Epoch 20, Train Loss: 0.0328, Test Loss: 0.0672, LR: 0.001000\n",
            "Epoch 30, Train Loss: 0.0250, Test Loss: 0.0577, LR: 0.001000\n",
            "Epoch 40, Train Loss: 0.0194, Test Loss: 0.0540, LR: 0.001000\n",
            "Epoch 50, Train Loss: 0.0165, Test Loss: 0.0496, LR: 0.001000\n",
            "Epoch 60, Train Loss: 0.0150, Test Loss: 0.0478, LR: 0.001000\n",
            "Epoch 70, Train Loss: 0.0132, Test Loss: 0.0462, LR: 0.001000\n",
            "Epoch 80, Train Loss: 0.0124, Test Loss: 0.0451, LR: 0.001000\n",
            "Epoch 90, Train Loss: 0.0119, Test Loss: 0.0441, LR: 0.001000\n",
            "Epoch 100, Train Loss: 0.0119, Test Loss: 0.0456, LR: 0.001000\n",
            "Final test loss: 0.0456\n"
          ]
        }
      ]
    }
  ]
}