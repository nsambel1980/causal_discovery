{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPWbb7Xyxt1mxz4iFhNgOr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nsambel1980/causal_discovery/blob/main/KS_hurst.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hurst Exponent estimation via KS test"
      ],
      "metadata": {
        "id": "S6-e68N4Qwr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "from typing import List, Tuple\n",
        "import warnings\n",
        "\n",
        "class KSHurstEstimator:\n",
        "    \"\"\"\n",
        "    Implements Kolmogorov-Smirnov based Hurst exponent estimation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, min_tau: int = 2, max_tau: int = None):\n",
        "        \"\"\"\n",
        "        Initialize the estimator.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        min_tau : int\n",
        "            Minimum scale to consider\n",
        "        max_tau : int\n",
        "            Maximum scale to consider (if None, will be set based on data length)\n",
        "        \"\"\"\n",
        "        self.min_tau = min_tau\n",
        "        self.max_tau = max_tau\n",
        "\n",
        "    def _get_increments_ghe(self, data: np.ndarray, tau: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Calculate increments for GHE method.\n",
        "        \"\"\"\n",
        "        increments = np.abs(data[tau:] - data[:-tau])\n",
        "        return increments\n",
        "\n",
        "    def _get_tau_range(self, data_length: int) -> List[int]:\n",
        "        \"\"\"\n",
        "        Generate range of tau values to use in estimation.\n",
        "        \"\"\"\n",
        "        max_tau = self.max_tau or data_length // 8\n",
        "        tau_range = []\n",
        "        current_tau = self.min_tau\n",
        "        while current_tau <= max_tau:\n",
        "            tau_range.append(current_tau)\n",
        "            current_tau *= 2\n",
        "        return tau_range\n",
        "\n",
        "    def _scale_samples(self, samples: np.ndarray, tau: float, h: float) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Scale samples by tau^H.\n",
        "        \"\"\"\n",
        "        return samples / (tau ** h)\n",
        "\n",
        "    def _compute_ks_statistic(self, data: np.ndarray, h: float, tau_range: List[int]) -> Tuple[float, List[float]]:\n",
        "        \"\"\"\n",
        "        Compute sum of KS statistics and p-values for given H value.\n",
        "        \"\"\"\n",
        "        # Get reference distribution (tau=1)\n",
        "        ref_samples = self._get_increments_ghe(data, 1)\n",
        "\n",
        "        total_ks_stat = 0\n",
        "        p_values = []\n",
        "\n",
        "        for tau in tau_range:\n",
        "            # Get and scale samples for current tau\n",
        "            current_samples = self._get_increments_ghe(data, tau)\n",
        "            scaled_samples = self._scale_samples(current_samples, tau, h)\n",
        "\n",
        "            # Compute KS statistic and p-value\n",
        "            ks_stat, p_value = stats.ks_2samp(ref_samples, scaled_samples)\n",
        "            total_ks_stat += ks_stat\n",
        "            p_values.append(p_value)\n",
        "\n",
        "        return total_ks_stat, p_values\n",
        "\n",
        "    def _compute_confidence_interval(self, data: np.ndarray, h_range: np.ndarray,\n",
        "                                tau_range: List[int], confidence_level: float = 0.99) -> Tuple[List[float], List[float]]:\n",
        "        \"\"\"\n",
        "        Compute confidence interval for H based on KS test p-values.\n",
        "        Returns all H values where self-similarity property holds at given confidence level.\n",
        "        \"\"\"\n",
        "        valid_h_values = []\n",
        "        all_p_values = []\n",
        "\n",
        "        # Test self-similarity for each H value\n",
        "        alpha = 1 - confidence_level\n",
        "        for h in h_range:\n",
        "            _, p_values = self._compute_ks_statistic(data, h, tau_range)\n",
        "            all_p_values.append(min(p_values))  # Store minimum p-value for this H\n",
        "\n",
        "            # If all scales pass KS test at this H, include it\n",
        "            if all(p >= alpha for p in p_values):\n",
        "                valid_h_values.append(h)\n",
        "\n",
        "        if not valid_h_values:\n",
        "            warnings.warn(\"No H values satisfy self-similarity at specified confidence level\")\n",
        "            return [h_range[np.argmax(all_p_values)]], all_p_values\n",
        "\n",
        "        return valid_h_values, all_p_values\n",
        "\n",
        "    def _optimize_h(self, data: np.ndarray, tau_range: List[int],\n",
        "                   h_range: np.ndarray) -> Tuple[float, np.ndarray, List[List[float]]]:\n",
        "        \"\"\"\n",
        "        Find H that minimizes sum of KS statistics.\n",
        "        \"\"\"\n",
        "        ks_stats = []\n",
        "        all_p_values = []\n",
        "        for h in h_range:\n",
        "            ks_stat, p_values = self._compute_ks_statistic(data, h, tau_range)\n",
        "            ks_stats.append(ks_stat)\n",
        "            all_p_values.append(p_values)\n",
        "\n",
        "        h_optimal = h_range[np.argmin(ks_stats)]\n",
        "        return h_optimal, np.array(ks_stats), all_p_values\n",
        "\n",
        "    def estimate(self, data: np.ndarray, h_steps: int = 100, confidence_level: float = 0.99) -> Tuple[float, dict]:\n",
        "        \"\"\"\n",
        "        Estimate Hurst exponent using KS method.\n",
        "        \"\"\"\n",
        "        if len(data) < 100:\n",
        "            warnings.warn(\"Time series may be too short for reliable estimation\")\n",
        "\n",
        "        tau_range = self._get_tau_range(len(data))\n",
        "        h_range = np.linspace(0.01, 0.99, h_steps)\n",
        "\n",
        "        # Find optimal H\n",
        "        h_estimate, ks_stats, all_p_values = self._optimize_h(data, tau_range, h_range)\n",
        "\n",
        "        # Compute confidence interval using KS test p-values\n",
        "        valid_h_values, p_values_by_h = self._compute_confidence_interval(\n",
        "            data, h_range, tau_range, confidence_level)\n",
        "\n",
        "        if len(valid_h_values) > 1:\n",
        "            ci = [min(valid_h_values), max(valid_h_values)]\n",
        "        else:\n",
        "            ci = valid_h_values * 2\n",
        "\n",
        "        results = {\n",
        "            'h_range': h_range,\n",
        "            'ks_statistics': ks_stats,\n",
        "            'p_values': all_p_values[np.argmin(ks_stats)],\n",
        "            'confidence_interval': ci,\n",
        "            'tau_range': tau_range,\n",
        "            'valid_h_values': valid_h_values,\n",
        "            'p_values_by_h': p_values_by_h,\n",
        "            'confidence_level': confidence_level\n",
        "        }\n",
        "\n",
        "        return h_estimate, results\n",
        "\n",
        "    def test_self_similarity(self, data: np.ndarray, h: float, confidence_level: float = 0.99) -> Tuple[bool, dict]:\n",
        "        \"\"\"\n",
        "        Test if the time series satisfies the self-similarity property for a given H.\n",
        "        \"\"\"\n",
        "        tau_range = self._get_tau_range(len(data))\n",
        "        _, p_values = self._compute_ks_statistic(data, h, tau_range)\n",
        "\n",
        "        # A series is self-similar if all KS tests accept the null hypothesis\n",
        "        alpha = 1 - confidence_level\n",
        "        is_self_similar = all(p >= alpha for p in p_values)\n",
        "\n",
        "        # Calculate overall confidence level\n",
        "        overall_confidence = confidence_level ** len(tau_range)\n",
        "\n",
        "        results = {\n",
        "            'tau_range': tau_range,\n",
        "            'p_values': p_values,\n",
        "            'individual_confidence_level': confidence_level,\n",
        "            'overall_confidence_level': overall_confidence,\n",
        "            'critical_value': alpha,\n",
        "            'failing_scales': [tau for tau, p in zip(tau_range, p_values) if p < alpha]\n",
        "        }\n",
        "\n",
        "        return is_self_similar, results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Generate some sample data (random walk, H should be ~0.5)\n",
        "    np.random.seed(40)\n",
        "    n_points = 1000\n",
        "    random_walk = np.cumsum(np.random.randn(n_points))\n",
        "\n",
        "    # Create estimator\n",
        "    estimator = KSHurstEstimator()\n",
        "\n",
        "    # Estimate H\n",
        "    h_estimate, estimation_results = estimator.estimate(random_walk)\n",
        "    print(f\"\\nEstimated Hurst exponent: {h_estimate:.3f}\")\n",
        "    print(f\"Confidence interval: [{estimation_results['confidence_interval'][0]:.3f}, \"\n",
        "          f\"{estimation_results['confidence_interval'][1]:.3f}]\")\n",
        "\n",
        "    # Test self-similarity at different confidence levels\n",
        "    confidence_levels = [0.99, 0.95, 0.90]\n",
        "    for cl in confidence_levels:\n",
        "        is_self_similar, test_results = estimator.test_self_similarity(random_walk, h_estimate, cl)\n",
        "        print(f\"\\nSelf-similarity test at {cl*100}% confidence level:\")\n",
        "        print(f\"Is self-similar: {is_self_similar}\")\n",
        "        if not is_self_similar:\n",
        "            print(f\"Failing scales (tau): {test_results['failing_scales']}\")\n",
        "        print(f\"Overall confidence level: {test_results['overall_confidence_level']:.3f}\")\n",
        "        print(f\"P-values at each scale: {[f'{p:.3f}' for p in test_results['p_values']]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HPLDo5vTzQW",
        "outputId": "de8fa3ce-c861-46f3-f385-ef5b5dfc8b0d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Estimated Hurst exponent: 0.515\n",
            "Confidence interval: [0.495, 0.535]\n",
            "\n",
            "Self-similarity test at 99.0% confidence level:\n",
            "Is self-similar: True\n",
            "Overall confidence level: 0.941\n",
            "P-values at each scale: ['0.489', '0.320', '0.174', '0.381', '0.325', '0.690']\n",
            "\n",
            "Self-similarity test at 95.0% confidence level:\n",
            "Is self-similar: True\n",
            "Overall confidence level: 0.735\n",
            "P-values at each scale: ['0.489', '0.320', '0.174', '0.381', '0.325', '0.690']\n",
            "\n",
            "Self-similarity test at 90.0% confidence level:\n",
            "Is self-similar: True\n",
            "Overall confidence level: 0.531\n",
            "P-values at each scale: ['0.489', '0.320', '0.174', '0.381', '0.325', '0.690']\n"
          ]
        }
      ]
    }
  ]
}